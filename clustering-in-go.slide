Clustering in Go
May 2016
Tags: go golang 

Wilfried Schobeiri
MediaMath
@nphase

* Who am I?
- Go enthusiast
- These days, mostly codes for fun
- Focused on Infrastruture & Platform @ [[http://careers.mediamath.com][MediaMath]]
- We're hiring!

* Why Go?

- Easy to build services
- Great stdlib
- Lot's of community libraries & utilities
- Great built-in tooling (like go fmt, test, vet, -race, etc) 
- Compiles as fast as a scripting language
- Just "feels" productive
- (This is not a language pitch talk)

* Why "Clustering in Go"?

* Why "Clustering in Go"?

- Clustering is not batteries included in Golang
- Lots of newer libraries, none very mature
- More often not, services roll it themselves

- So, here's one way of building a clustered, stateful service in Go.
*This*slide*is*incomplete*


* And now, for a (fake-ish) scenario

- Multiple datacenters
- Separated by thousands of miles each (eg, ORD - HKG - AMS),
- With many events happening concurrently at each one.

- *We*want*to*count*them.*

* With some constraints:

- Counting should be *fast*, we can't afford to cross the ocean every time
- Counts should be *correct* (please don't lose my events)

Starting to look like an AP system, right?


* Let's get started

First, a basic counter service

- One node
- Counter = Atomic Int
- Nothin Fancy

 $ curl http://localhost:4000/

 0

 $ curl http://localhost:4000/inc?amount=1
 
 1 


* A basic Counter Service


.play simple_counter/main.go /START OMIT/,/END OMIT/


* Ok, let's geodistribute it.

* Ok, let's geodistribute it.

- A node (or several) in each datacenter
- Increment requests routed to the closest node

Let's stand one up in each.

* Demo (3 nodes, each with diverging counters)

*This*slide*is*incomplete*

* Duh, we're not replicating state!

- We need the counters to talk to each other
- Which means we need the nodes to know about each other
- Which means we need to to solve for cluster membership

Enter, the [[https://github.com/hashicorp/memberlist][memberlist]] package

* Memberlist

- A Go library that manages cluster membership
- Based on [[https://www.cs.cornell.edu/~asdas/research/dsn02-swim.pdf][SWIM]], a gossip-style membership protocol
- Has baked in member failure detection
- Used by Consul, Docker, libnetwork, many more 

* About SWIM

"Scalable Weakly-consistent Infection-style Process Group Membership Protocol"

Two goals: 

- Maintain a local membership list of non-faulty processes
- Detect and eventually notify others of process failures

* SWIM mechanics

- Gossip-based
- On join, a new node does a full state sync with an existing member, and begins gossipping its existence to the cluster
- Gossip about memberlist state happens on a regular interval and against number of randomly selected members
- If a node doesn't ack a probe message, it is marked "suspicious"
- If a suspicious node doesn't dispute suspicion after a timeout, it's marked dead
- Every so often, a full state sync is done between random members (expensive!)
- Tradeoffs between bandwidth and convergence time are configurable

More details about SWIM can be found [[https://www.cs.cornell.edu/~asdas/research/dsn02-swim.pdf][here]] and [[http://prakhar.me/articles/swim/][here]].

* Becoming Cluster-aware via Memberlist

	members  = flag.String("members", "", "comma seperated list of members")

	...

	c := memberlist.DefaultWANConfig()
	
	m, err := memberlist.Create(c)
	
	if err != nil {
		return err
	}
	
	//Join other members if specified, otherwise start a new cluster
	if len(*members) > 0 {
		members_each := strings.Split(*members, ",")
		_, err := m.Join(members_each)
		if err != nil {
			return err
		}
	}

	


* Demo 

#Demonstrate /cluster
#Kill a node, refresh /cluster
#still not merging!


* CRDTs to the rescue!

* CRDTs, simplified

#semilattice algebra lets them merbe without conflict


- CRDT = Conflict-Free Replicated Data Types
- Counters, Sets, Maps, et al
- Operations within the type must be associative, commutative, and idempotent 
- Order-free
- Therefore, very easy to handle failure scenarios: just retry the merge!

#network down? merge again! messages out of order? merge again! state out of sync? merge again!

CRDTs are by nature eventually consistent, because there is no single source of truth.

Some notes can be found [[http://hal.upmc.fr/inria-00555588/document][here]] and [[https://github.com/pfraze/crdt_notes][here]] (among many others!). 

* G-Counter

Perhaps one of the most basic CRDTs:

- A counter with only two ops: increment and merge (no decrement!)
- Each node manages its own count
- Nodes communicate their counter state with other nodes
- Merges take the max() count for each node

G-Counter's Value is the sum of all node count values


#why go through all the trouble? when i can just query locally?
#what if the node goes down permanently? waht if you lose the state?

* 

	type GCounter struct {
		// ident provides a unique identity to each replica.
		ident string

		// counter maps identity of each replica to their counter values
		counter map[string]int
	}

	func (g *GCounter) IncVal(incr int) {
		g.counter[g.ident] += incr
	}

	func (g *GCounter) Count() (total int) {
		for _, val := range g.counter {
			total += val
		}
		return 
	}

	func (g *GCounter) Merge(c *GCounter) {
		for ident, val := range c.counter {
			if v, ok := g.counter[ident]; !ok || v < val {
				g.counter[ident] = val
			}
		}
	}

* Demo 

#Demonstrate /verbose
#still not merging!




* Let's Merge State!

* Merging State via Memberlist

   [DEBUG] memberlist: Initiating push/pull sync with: 127.0.0.1:61300

- Memberlist does a "push/pull" to do a complete state exchange with another random member
- We can piggyback this state exchange via the `Delegate` interface: `LocalState()` and `MergeRemoteState()` 
- Push/pull interval is configurable
- Happens over TCP

Let's use it to eventually merge state in the background.


* Merging State via Memberlist

	// Share the local counter state via MemberList to another node
	func (d *delegate) LocalState(join bool) []byte {

		b, err := counter.MarshalJSON()

		if err != nil {
			panic(err)
		}

		return b
	}

	// Merge a received counter state
	func (d *delegate) MergeRemoteState(buf []byte, join bool) {
		if len(buf) == 0 {
			return
		}

		externalCRDT := crdt.NewGCounterFromJSON(buf)
		counter.Merge(externalCRDT)
	}

* Demo

* Success! (Eventually)

* Want so sync faster? 

It's possible to broadcast to all member nodes, via `NotifyMsg()` and `QueueBroadcast()`. 

	func BroadcastState() {
		...
		broadcasts.QueueBroadcast(&broadcast{
			msg:    b,
		})
	}

	// NotifyMsg is invoked upon receipt of message
	func (d *delegate) NotifyMsg(b []byte) {
		...
		switch update.Action {
		case "merge":
			externalCRDT := crdt.NewGCounterFromJSONBytes(update.Data)
			counter.Merge(externalCRDT)
		}
		...
	}

Faster sync for more bandwidth. Still eventually consistent.

* Demo

* Next:

- Implement persistence and time windowing
- No tests? For shame!
- We probably want more than one node per datacenter
- Jepsen all the things
- Implement a real RPC layer instead of Memberlist' `delegate` for finer performance and authn/z control
- Run it as a unikernel within docker running inside a VM in the cloud
- Sprinkle some devops magic dust on it
- Achieve peak microservice



* Fin!

- Questions?
- Slides and code can be found at [[github.com/nphase/go-clustering-example][github.com/nphase/go-clustering-example]]
- MediaMath is hiring!
- Thanks!

